{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "e21d2c03",
      "metadata": {
        "id": "e21d2c03"
      },
      "source": [
        "# **Marketing Spend Optimization using Machine Learning in Python**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "8ed28017",
      "metadata": {
        "id": "8ed28017"
      },
      "source": [
        "# **Introduction**\n",
        "\n",
        "A company specializing in B2C sales (product: Data Science and Data Engineering related courses) spends significant money on marketing campaigns across various channels such as social media, email, and search advertising. More information on marketing sources can be found in the data column “Marketing Source”. The marketing team faces challenges in optimizing the marketing budget allocation across these channels to maximize revenue and return on investment (ROI).<br><br>\n",
        "This project aims to develop a data-driven approach to optimize the marketing budget allocation across various channels to maximize revenue and ROI. By using machine learning algorithms, the marketing team can make informed decisions about allocating the marketing budget based on predicted revenue and ROI.\n",
        "\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "4ef2ce3b",
      "metadata": {
        "id": "4ef2ce3b"
      },
      "source": [
        "### **Business Impact of Marketing Budget Optimization**\n",
        "\n",
        "\n",
        "**Increase product conversions**: Marketing Budget Optimization leads to righ user targeting through right channels/assets which leads to better conversions.\n",
        "\n",
        "\n",
        "\n",
        "**Increase revenue**: Increased conversions(as mentioned in point above) will lead to more revenue or buyer engagement. For example, if the company is able to target a user who is more active on Instagram, chances are more that he/she will click on the Ad and add the product to cart. So overall probability of an order increases and hence the revenue.\n",
        "\n",
        "\n",
        "\n",
        "**Improve budget allocation**: Over budgeting on non-efficient channels lead to waste of marketing money without getting enough revenue.\n",
        "\n",
        "**Improve Customer Acquisition Cost**: Customer Acquisition Cost(CAC) improves if right targeting channels are used for a customer often leading to better repeat rates as well.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "5b84de32",
      "metadata": {
        "id": "5b84de32"
      },
      "source": [
        "### **Assumptions**\n",
        "\n",
        "* We assume that <b>Interest Level</b> is our target variable which refers to the interest of a user for a lead id\n",
        "* We assume that whenever the target variable is NA or Not called, the corresponding lead id is not meaningful and hence dropped\n",
        "* If the model's prediction is very close to 1, it means that the user is very likely to engage with the lead id\n",
        "* Columns with a lot of null values are not meaningful and imputation also won't be helpful"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "8d5147ac",
      "metadata": {
        "id": "8d5147ac"
      },
      "source": [
        "## **Approach**\n",
        "\n",
        "\n",
        "We are treating this problem as an supervised learning problem. So every data point will have a target variable for the model to learn the dependencies and predict on the unknown.\n",
        "\n",
        "\n",
        "In real life, this model would tell the business whether a user is likely to engage with the ad or not and that would in turn help the company to allocate budgets accordingly.\n",
        "\n",
        "\n",
        "Given our assumptions about the data, we will build a prediction model based on the historical data. Simplifying, here's the logic of what we'll build:\n",
        "\n",
        "\n",
        "1. We'll build a model to identify if a customer will be interested in the lead;\n",
        "2. We'll use various tree based model and compare their performance on interest prediction;\n",
        "3. We will then choose the most successful model to use in production;\n",
        "\n",
        "* Exploratory Data Analysis (EDA):\n",
        "  * Understand the features and their relationships with target variables\n",
        "  * Check for missing or invalid values and their imputation\n",
        "\n",
        "\n",
        "* Data Preprocessing:\n",
        "  * Encode the variables using label encoding\n",
        "  * Split the dataset into training and testing sets\n",
        "\n",
        "* Model Building and Testing:\n",
        "  * Random Forest\n",
        "  * Light Gradient Boosting\n",
        "  * Extreme Gradient Boosting\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "c4fc1bfa",
      "metadata": {
        "id": "c4fc1bfa"
      },
      "source": [
        "# **Package Requirements**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "qGeRqhOz_KVv",
      "metadata": {
        "id": "qGeRqhOz_KVv"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "b124c0ed",
      "metadata": {
        "id": "b124c0ed"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn import preprocessing\n",
        "# Suppress all warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "7ba55589",
      "metadata": {
        "id": "7ba55589"
      },
      "source": [
        "# **The Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "588443c9",
      "metadata": {
        "id": "588443c9"
      },
      "outputs": [],
      "source": [
        "csv_file_path = \"D:\\Github\\ML-Powered-Marketing-Spend-Optimization\\Marketing_Data.csv\"\n",
        "df = pd.read_csv(csv_file_path)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "44be0c30",
      "metadata": {
        "id": "44be0c30"
      },
      "source": [
        "Lets look at the first 10 records from the dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "65e0aadd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 878
        },
        "id": "65e0aadd",
        "outputId": "c8b3c95c-b659-48f7-aa19-5f89bf969f35",
        "scrolled": true
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Lead Id</th>\n",
              "      <th>Lead Owner</th>\n",
              "      <th>Interest Level</th>\n",
              "      <th>Lead created</th>\n",
              "      <th>Lead Location(Auto)</th>\n",
              "      <th>Creation Source</th>\n",
              "      <th>Next activity</th>\n",
              "      <th>What do you do currently ?</th>\n",
              "      <th>What are you looking for in Product ?</th>\n",
              "      <th>Website Source</th>\n",
              "      <th>Lead Last Update time</th>\n",
              "      <th>Marketing Source</th>\n",
              "      <th>Lead Location(Manual)</th>\n",
              "      <th>Demo Date</th>\n",
              "      <th>Demo Status</th>\n",
              "      <th>Closure date</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5e502dcf828b8975a78e89f3e9aeac12</td>\n",
              "      <td>e14c3a</td>\n",
              "      <td>Not Interested</td>\n",
              "      <td>12-01-2023 16:42</td>\n",
              "      <td>IN</td>\n",
              "      <td>API</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Student</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>12-01-2023 19:27</td>\n",
              "      <td>NaN</td>\n",
              "      <td>India</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>efe3f074c61959c2ea1906dd0346aa69</td>\n",
              "      <td>d16267</td>\n",
              "      <td>Slightly Interested</td>\n",
              "      <td>04-12-2021 09:32</td>\n",
              "      <td>NaN</td>\n",
              "      <td>API</td>\n",
              "      <td>12-01-2022 00:00</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Sales lead</td>\n",
              "      <td>12-01-2022 17:17</td>\n",
              "      <td>Paid - Instagram</td>\n",
              "      <td>India</td>\n",
              "      <td>05-12-2021 00:00</td>\n",
              "      <td>No Show</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>d26dc5cd5843622a203cf396b4ee4b1a</td>\n",
              "      <td>d138f9</td>\n",
              "      <td>No Answer</td>\n",
              "      <td>15-04-2022 10:16</td>\n",
              "      <td>NaN</td>\n",
              "      <td>API</td>\n",
              "      <td>16-04-2022 00:00</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>16-04-2022 20:35</td>\n",
              "      <td>Paid-Adwords</td>\n",
              "      <td>In</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>d50acaedc1e5b9c18f8ceb3c6cff345b</td>\n",
              "      <td>38e2a6</td>\n",
              "      <td>Not Interested</td>\n",
              "      <td>21-10-2022 17:02</td>\n",
              "      <td>IN</td>\n",
              "      <td>API</td>\n",
              "      <td>23-10-2022 00:00</td>\n",
              "      <td>fresher</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>02-12-2022 13:35</td>\n",
              "      <td>Paid-Adwords</td>\n",
              "      <td>IN</td>\n",
              "      <td>22-11-2022 00:00</td>\n",
              "      <td>Scheduled</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>07758f3d12a23e68bb3b58b8009dd9a8</td>\n",
              "      <td>d130bb</td>\n",
              "      <td>Not Interested</td>\n",
              "      <td>25-10-2021 10:48</td>\n",
              "      <td>NaN</td>\n",
              "      <td>API</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Sales lead</td>\n",
              "      <td>13-11-2021 14:51</td>\n",
              "      <td>Affiliate</td>\n",
              "      <td>India</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>665eb8f7c975b055afa58b5dda3a78bc</td>\n",
              "      <td>d5b5bd</td>\n",
              "      <td>Slightly Interested</td>\n",
              "      <td>24-11-2022 22:41</td>\n",
              "      <td>NaN</td>\n",
              "      <td>API</td>\n",
              "      <td>26-11-2022 00:00</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Big Data engineering</td>\n",
              "      <td>NaN</td>\n",
              "      <td>26-11-2022 19:49</td>\n",
              "      <td>NaN</td>\n",
              "      <td>TR</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>a1ea99cba3b88f6c59fea8a84f051dec</td>\n",
              "      <td>d16267</td>\n",
              "      <td>No Answer</td>\n",
              "      <td>07-07-2022 14:50</td>\n",
              "      <td>NaN</td>\n",
              "      <td>API</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Sales lead</td>\n",
              "      <td>08-07-2022 18:20</td>\n",
              "      <td>Medium</td>\n",
              "      <td>India</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>e69523450132baed2dd72836cdfc9778</td>\n",
              "      <td>d130bb</td>\n",
              "      <td>Not Interested</td>\n",
              "      <td>16-09-2021 23:37</td>\n",
              "      <td>NaN</td>\n",
              "      <td>API</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Glass maker at home</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Sales lead</td>\n",
              "      <td>12-11-2021 04:49</td>\n",
              "      <td>Paid - Facebook</td>\n",
              "      <td>India</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>fe244887bc37b5f49311c750ce6b279f</td>\n",
              "      <td>d138f9</td>\n",
              "      <td>No Answer</td>\n",
              "      <td>08-06-2022 13:30</td>\n",
              "      <td>NaN</td>\n",
              "      <td>API</td>\n",
              "      <td>24-06-2022 00:00</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>24-06-2022 10:44</td>\n",
              "      <td>Paid - Instagram</td>\n",
              "      <td>In</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>3500a29dc4849a7166e98db2e44ddc53</td>\n",
              "      <td>38e2a6</td>\n",
              "      <td>No Answer</td>\n",
              "      <td>21-10-2022 23:50</td>\n",
              "      <td>IN</td>\n",
              "      <td>API</td>\n",
              "      <td>23-10-2022 00:00</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>23-10-2022 11:09</td>\n",
              "      <td>Paid - Instagram</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                            Lead Id Lead Owner       Interest Level  \\\n",
              "0  5e502dcf828b8975a78e89f3e9aeac12     e14c3a       Not Interested   \n",
              "1  efe3f074c61959c2ea1906dd0346aa69     d16267  Slightly Interested   \n",
              "2  d26dc5cd5843622a203cf396b4ee4b1a     d138f9            No Answer   \n",
              "3  d50acaedc1e5b9c18f8ceb3c6cff345b     38e2a6       Not Interested   \n",
              "4  07758f3d12a23e68bb3b58b8009dd9a8     d130bb       Not Interested   \n",
              "5  665eb8f7c975b055afa58b5dda3a78bc     d5b5bd  Slightly Interested   \n",
              "6  a1ea99cba3b88f6c59fea8a84f051dec     d16267            No Answer   \n",
              "7  e69523450132baed2dd72836cdfc9778     d130bb       Not Interested   \n",
              "8  fe244887bc37b5f49311c750ce6b279f     d138f9            No Answer   \n",
              "9  3500a29dc4849a7166e98db2e44ddc53     38e2a6            No Answer   \n",
              "\n",
              "       Lead created Lead Location(Auto) Creation Source     Next activity  \\\n",
              "0  12-01-2023 16:42                  IN             API               NaN   \n",
              "1  04-12-2021 09:32                 NaN             API  12-01-2022 00:00   \n",
              "2  15-04-2022 10:16                 NaN             API  16-04-2022 00:00   \n",
              "3  21-10-2022 17:02                  IN             API  23-10-2022 00:00   \n",
              "4  25-10-2021 10:48                 NaN             API               NaN   \n",
              "5  24-11-2022 22:41                 NaN             API  26-11-2022 00:00   \n",
              "6  07-07-2022 14:50                 NaN             API               NaN   \n",
              "7  16-09-2021 23:37                 NaN             API               NaN   \n",
              "8  08-06-2022 13:30                 NaN             API  24-06-2022 00:00   \n",
              "9  21-10-2022 23:50                  IN             API  23-10-2022 00:00   \n",
              "\n",
              "  What do you do currently ? What are you looking for in Product ?  \\\n",
              "0                    Student                                   NaN   \n",
              "1                        NaN                                   NaN   \n",
              "2                        NaN                                   NaN   \n",
              "3                    fresher                                   NaN   \n",
              "4                        NaN                                   NaN   \n",
              "5                        NaN                  Big Data engineering   \n",
              "6                        NaN                                   NaN   \n",
              "7        Glass maker at home                                   NaN   \n",
              "8                        NaN                                   NaN   \n",
              "9                        NaN                                   NaN   \n",
              "\n",
              "  Website Source Lead Last Update time  Marketing Source  \\\n",
              "0            NaN      12-01-2023 19:27               NaN   \n",
              "1     Sales lead      12-01-2022 17:17  Paid - Instagram   \n",
              "2            NaN      16-04-2022 20:35      Paid-Adwords   \n",
              "3            NaN      02-12-2022 13:35      Paid-Adwords   \n",
              "4     Sales lead      13-11-2021 14:51         Affiliate   \n",
              "5            NaN      26-11-2022 19:49               NaN   \n",
              "6     Sales lead      08-07-2022 18:20            Medium   \n",
              "7     Sales lead      12-11-2021 04:49   Paid - Facebook   \n",
              "8            NaN      24-06-2022 10:44  Paid - Instagram   \n",
              "9            NaN      23-10-2022 11:09  Paid - Instagram   \n",
              "\n",
              "  Lead Location(Manual)         Demo Date Demo Status Closure date  \n",
              "0                 India               NaN         NaN          NaN  \n",
              "1                 India  05-12-2021 00:00     No Show          NaN  \n",
              "2                    In               NaN         NaN          NaN  \n",
              "3                    IN  22-11-2022 00:00   Scheduled          NaN  \n",
              "4                 India               NaN         NaN          NaN  \n",
              "5                    TR               NaN         NaN          NaN  \n",
              "6                 India               NaN         NaN          NaN  \n",
              "7                 India               NaN         NaN          NaN  \n",
              "8                    In               NaN         NaN          NaN  \n",
              "9                   NaN               NaN         NaN          NaN  "
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head(10)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "472db6f4",
      "metadata": {
        "id": "472db6f4"
      },
      "source": [
        "From the dataset preview, it's clear that several fields such as \"Demo Date,\" \"Closure date,\" and \"What are you looking for in Product?\" contain a high number of missing values, which may limit their usefulness unless carefully imputed or excluded. Key categorical features like \"Marketing Source\" and \"Lead Owner\" show enough variability to serve as strong predictors in a classification model. Additionally, the target variable \"Interest Level\" includes values like \"Not Interested,\" \"Slightly Interested,\" and \"No Answer,\" indicating the need for thoughtful preprocessing—such as binarization—to ensure the model can learn meaningful patterns."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "819361ed",
      "metadata": {
        "id": "819361ed"
      },
      "source": [
        "# **Exploratory Data Analysis**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "44b12f3d",
      "metadata": {
        "id": "44b12f3d"
      },
      "source": [
        "### **What can we learn from the data?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "e749623a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e749623a",
        "outputId": "96dc0ff5-8adc-4054-f86b-09ab5062751c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(38984, 16)"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "f887b4d9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f887b4d9",
        "outputId": "a5d6d6f0-6b00-4d14-f042-9f87d31c192b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['Lead Id', 'Lead Owner', 'Interest Level', 'Lead created',\n",
              "       'Lead Location(Auto)', 'Creation Source', 'Next activity',\n",
              "       'What do you do currently ?', 'What are you looking for in Product ?',\n",
              "       'Website Source', 'Lead Last Update time', 'Marketing Source',\n",
              "       'Lead Location(Manual)', 'Demo Date', 'Demo Status', 'Closure date'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Checking the names of the columns\n",
        "df.columns"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "14e94664",
      "metadata": {
        "id": "14e94664"
      },
      "source": [
        "## **Data Dictionary**\n",
        "\n",
        "\n",
        "\n",
        "| Column name\t | Description|\n",
        "| ----- | ----- |\n",
        "| Lead Id|  Unique Identifier |\n",
        "| Lead Owner|  Internal sales person associated with the lead |\n",
        "| Interest Level|  What is lead's interest level? (entered manually) |\n",
        "| Lead created|  Lead creation date |\n",
        "| Lead Location(Auto)|  Automatically detected location |\n",
        "| Creation Source|  Creation source of the lead |\n",
        "| Next activity|  Date for Next Activity |\n",
        "| What do you do currently ?|  Current profile of lead |\n",
        "| What are you looking for in Product ?|  Specific requirement from product |\n",
        "| Website Source|  Website Source of the Lead |\n",
        "| Lead Last Update time|  Last update time for Lead |\n",
        "| Marketing Source|  Marketing Source of the Lead |\n",
        "| Lead Location(Manual)|  Manually entered lead location |\n",
        "| Demo Date|  Date for Demo |\n",
        "| Demo Status|  Status of demo booked with lead |\n",
        "| Closure date|  Lead closing date |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "8beee630",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "id": "8beee630",
        "outputId": "c4a967e6-ae2c-4fc6-80c8-0a02ca7b5da1"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Lead Id</th>\n",
              "      <th>Lead Owner</th>\n",
              "      <th>Interest Level</th>\n",
              "      <th>Lead created</th>\n",
              "      <th>Lead Location(Auto)</th>\n",
              "      <th>Creation Source</th>\n",
              "      <th>Next activity</th>\n",
              "      <th>What do you do currently ?</th>\n",
              "      <th>What are you looking for in Product ?</th>\n",
              "      <th>Website Source</th>\n",
              "      <th>Lead Last Update time</th>\n",
              "      <th>Marketing Source</th>\n",
              "      <th>Lead Location(Manual)</th>\n",
              "      <th>Demo Date</th>\n",
              "      <th>Demo Status</th>\n",
              "      <th>Closure date</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>38984</td>\n",
              "      <td>38984</td>\n",
              "      <td>38847</td>\n",
              "      <td>38984</td>\n",
              "      <td>10810</td>\n",
              "      <td>38984</td>\n",
              "      <td>14776</td>\n",
              "      <td>16909</td>\n",
              "      <td>9970</td>\n",
              "      <td>24088</td>\n",
              "      <td>38984</td>\n",
              "      <td>28339</td>\n",
              "      <td>34974</td>\n",
              "      <td>10851</td>\n",
              "      <td>11423</td>\n",
              "      <td>629</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>37450</td>\n",
              "      <td>23</td>\n",
              "      <td>8</td>\n",
              "      <td>35951</td>\n",
              "      <td>169</td>\n",
              "      <td>3</td>\n",
              "      <td>2610</td>\n",
              "      <td>6831</td>\n",
              "      <td>4046</td>\n",
              "      <td>10</td>\n",
              "      <td>32693</td>\n",
              "      <td>46</td>\n",
              "      <td>415</td>\n",
              "      <td>583</td>\n",
              "      <td>3</td>\n",
              "      <td>277</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>bcbcf737090f0a52c59237fb0ee921d5</td>\n",
              "      <td>2f6f7f</td>\n",
              "      <td>Slightly Interested</td>\n",
              "      <td>13-01-2022 14:05</td>\n",
              "      <td>IN</td>\n",
              "      <td>API</td>\n",
              "      <td>31-01-2023 00:00</td>\n",
              "      <td>Student</td>\n",
              "      <td>DS</td>\n",
              "      <td>Sales lead</td>\n",
              "      <td>06-03-2023 17:53</td>\n",
              "      <td>SEO</td>\n",
              "      <td>IN</td>\n",
              "      <td>30-07-2022 00:00</td>\n",
              "      <td>Scheduled</td>\n",
              "      <td>01-05-2022 00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>6</td>\n",
              "      <td>5643</td>\n",
              "      <td>14572</td>\n",
              "      <td>17</td>\n",
              "      <td>6735</td>\n",
              "      <td>36291</td>\n",
              "      <td>74</td>\n",
              "      <td>3406</td>\n",
              "      <td>481</td>\n",
              "      <td>23121</td>\n",
              "      <td>401</td>\n",
              "      <td>10127</td>\n",
              "      <td>14126</td>\n",
              "      <td>48</td>\n",
              "      <td>4000</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                 Lead Id Lead Owner       Interest Level  \\\n",
              "count                              38984      38984                38847   \n",
              "unique                             37450         23                    8   \n",
              "top     bcbcf737090f0a52c59237fb0ee921d5     2f6f7f  Slightly Interested   \n",
              "freq                                   6       5643                14572   \n",
              "\n",
              "            Lead created Lead Location(Auto) Creation Source  \\\n",
              "count              38984               10810           38984   \n",
              "unique             35951                 169               3   \n",
              "top     13-01-2022 14:05                  IN             API   \n",
              "freq                  17                6735           36291   \n",
              "\n",
              "           Next activity What do you do currently ?  \\\n",
              "count              14776                      16909   \n",
              "unique              2610                       6831   \n",
              "top     31-01-2023 00:00                    Student   \n",
              "freq                  74                       3406   \n",
              "\n",
              "       What are you looking for in Product ? Website Source  \\\n",
              "count                                   9970          24088   \n",
              "unique                                  4046             10   \n",
              "top                                       DS     Sales lead   \n",
              "freq                                     481          23121   \n",
              "\n",
              "       Lead Last Update time Marketing Source Lead Location(Manual)  \\\n",
              "count                  38984            28339                 34974   \n",
              "unique                 32693               46                   415   \n",
              "top         06-03-2023 17:53              SEO                    IN   \n",
              "freq                     401            10127                 14126   \n",
              "\n",
              "               Demo Date Demo Status      Closure date  \n",
              "count              10851       11423               629  \n",
              "unique               583           3               277  \n",
              "top     30-07-2022 00:00   Scheduled  01-05-2022 00:00  \n",
              "freq                  48        4000                 9  "
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Check the Information of the Dataframe, number of unique values and frequency\n",
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "0ae9af3f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ae9af3f",
        "outputId": "f4d9c519-0c1e-402b-b17a-79274b7442bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 38984 entries, 0 to 38983\n",
            "Data columns (total 16 columns):\n",
            " #   Column                                 Non-Null Count  Dtype \n",
            "---  ------                                 --------------  ----- \n",
            " 0   Lead Id                                38984 non-null  object\n",
            " 1   Lead Owner                             38984 non-null  object\n",
            " 2   Interest Level                         38847 non-null  object\n",
            " 3   Lead created                           38984 non-null  object\n",
            " 4   Lead Location(Auto)                    10810 non-null  object\n",
            " 5   Creation Source                        38984 non-null  object\n",
            " 6   Next activity                          14776 non-null  object\n",
            " 7   What do you do currently ?             16909 non-null  object\n",
            " 8   What are you looking for in Product ?  9970 non-null   object\n",
            " 9   Website Source                         24088 non-null  object\n",
            " 10  Lead Last Update time                  38984 non-null  object\n",
            " 11  Marketing Source                       28339 non-null  object\n",
            " 12  Lead Location(Manual)                  34974 non-null  object\n",
            " 13  Demo Date                              10851 non-null  object\n",
            " 14  Demo Status                            11423 non-null  object\n",
            " 15  Closure date                           629 non-null    object\n",
            "dtypes: object(16)\n",
            "memory usage: 4.8+ MB\n"
          ]
        }
      ],
      "source": [
        "# Check the Information of the Dataframe, datatypes and non-null counts\n",
        "df.info()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "3bbf6183",
      "metadata": {
        "id": "3bbf6183"
      },
      "source": [
        "**Observation:**\n",
        "* we can see some null values present in this data. We will treat them later\n",
        "* There is also imbalance which must be addressed and features like Lead Owner, Marketing Source, and Website Source show good categorical diversity, making them potentially valuable inputs for a classification model predicting lead engagement or conversion.\n",
        "* Lead created, Next activity, Lead Last Update time and Demo Date should be datetime datatype but it is object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "efb1538f",
      "metadata": {
        "id": "efb1538f"
      },
      "outputs": [],
      "source": [
        "df['Lead created'] = pd.to_datetime(df['Lead created'], format=\"%d-%m-%Y %H:%M\")\n",
        "df['Lead Last Update time'] = pd.to_datetime(df['Lead Last Update time'], format=\"%d-%m-%Y %H:%M\")\n",
        "df['Next activity'] = pd.to_datetime(df['Next activity'], format=\"%d-%m-%Y %H:%M\")\n",
        "df['Demo Date'] = pd.to_datetime(df['Demo Date'], format=\"%d-%m-%Y %H:%M\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "a8753a62",
      "metadata": {
        "id": "a8753a62"
      },
      "source": [
        "Lets see how many different Lead Owners we have"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "2badf7cb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2badf7cb",
        "outputId": "30954b40-0274-49b1-df04-1e8ece7e61bd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['e14c3a', 'd16267', 'd138f9', '38e2a6', 'd130bb', 'd5b5bd',\n",
              "       '949886', 'fc348d', 'c18c01', '1eafbe', '2f6f7f', '5fe006',\n",
              "       '8a10c8', '1a9b5d', 'c5837c', '64c0b2', '684149', '154755',\n",
              "       'b89cfd', '8c20b0', '2c7db1', '65ed8c', '64347b'], dtype=object)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['Lead Owner'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "6bc9d26e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bc9d26e",
        "outputId": "2183ca10-314d-4fec-95f0-d95847cd958b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Lead Owner\n",
              "2f6f7f    5643\n",
              "d16267    5313\n",
              "1eafbe    5226\n",
              "d5b5bd    4417\n",
              "fc348d    3405\n",
              "1a9b5d    2515\n",
              "d138f9    2023\n",
              "e14c3a    1695\n",
              "c5837c    1415\n",
              "d130bb    1195\n",
              "b89cfd    1120\n",
              "949886     986\n",
              "8c20b0     667\n",
              "38e2a6     594\n",
              "684149     559\n",
              "c18c01     526\n",
              "5fe006     525\n",
              "8a10c8     509\n",
              "64c0b2     317\n",
              "2c7db1     288\n",
              "154755      34\n",
              "65ed8c      10\n",
              "64347b       2\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['Lead Owner'].value_counts()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "26ce5211",
      "metadata": {
        "id": "26ce5211"
      },
      "source": [
        "**Observation**\n",
        "* The data seems to be evenly distributed amongst lead owners"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "cee61b3f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cee61b3f",
        "outputId": "473b4dd3-757a-4193-ec62-74406dd68d7e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['Not Interested', 'Slightly Interested', 'No Answer', 'Closed',\n",
              "       'Not called', 'Invalid Number', 'Fairly Interested', nan,\n",
              "       'Very Interested'], dtype=object)"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['Interest Level'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "aca285ea",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aca285ea",
        "outputId": "de509c8c-f5ca-441b-8a3f-d8cc3487ba36"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Interest Level\n",
              "Slightly Interested    14572\n",
              "Not Interested         10545\n",
              "No Answer               9254\n",
              "Not called              1585\n",
              "Fairly Interested       1320\n",
              "Closed                   811\n",
              "Invalid Number           636\n",
              "Very Interested          124\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['Interest Level'].value_counts()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "84e6acc6",
      "metadata": {
        "id": "84e6acc6"
      },
      "source": [
        "**Observation**\n",
        "* We see that some of the interest levels are similar semantically\n",
        "* Since interest level is our target variable, it seems to be nicely distributed"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "7347fa30",
      "metadata": {
        "id": "7347fa30"
      },
      "source": [
        "**Points to ponder upon**\n",
        "* Should we formulate the problem as multi-class or binary classification problem?\n",
        "* In case, we want to do binary classification, how do we deal with many values in our target variable?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "afec9220",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afec9220",
        "outputId": "7b3adacd-6b28-471c-e36f-aa9a5aab3bfc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "What do you do currently ?\n",
              "Student                           3406\n",
              "student                           1282\n",
              "Fresher                            298\n",
              "Working                            194\n",
              "Working pro                        148\n",
              "                                  ... \n",
              "JFW                                  1\n",
              "BTECH last year                      1\n",
              "Asst Project                         1\n",
              "He is working in Sales               1\n",
              "Course AIMA - Business Analyst       1\n",
              "Name: count, Length: 6831, dtype: int64"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['What do you do currently ?'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "bffefd6d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bffefd6d",
        "outputId": "cc7c2a5c-1b54-42ae-a700-d600d1a642c4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(6832,)"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['What do you do currently ?'].unique().shape"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "a894bb3d",
      "metadata": {
        "id": "a894bb3d"
      },
      "source": [
        "**Observation**\n",
        "* There are many unique values in the above column\n",
        "* If we process the string we can reduce these"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "93659f59",
      "metadata": {
        "id": "93659f59"
      },
      "source": [
        "**Think about it**\n",
        "* Do we need to focus on so many values and confuse the model?\n",
        "* What can we do to reduce these?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "a3bbf1f6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3bbf1f6",
        "outputId": "71b5f333-1b0a-4929-ac9a-67c534ece622"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['API', 'Manually created', 'Deal'], dtype=object)"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['Creation Source'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "b88e1256",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b88e1256",
        "outputId": "80d74904-a0eb-446c-c731-f6865d7485f0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Creation Source\n",
              "API                 36291\n",
              "Manually created     2533\n",
              "Deal                  160\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['Creation Source'].value_counts()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "238271e7",
      "metadata": {
        "id": "238271e7"
      },
      "source": [
        "**Observation**\n",
        "* This feature looks well balanced in terms of unique values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "d943605e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d943605e",
        "outputId": "3e9cc48d-7c51-4910-de6b-96b00d3c667a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(4047,)"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['What are you looking for in Product ?'].unique().shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "88dfc533",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88dfc533",
        "outputId": "e694463f-88a2-40de-987b-4824eed9f6a2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "What are you looking for in Product ?\n",
              "DS                                  481\n",
              "ML                                  325\n",
              "DS projects                         254\n",
              "ML projects                         221\n",
              "BD                                  158\n",
              "                                   ... \n",
              "Better knowledge & hands exp          1\n",
              "ds in shipping and logistics          1\n",
              "Better Knowledge & Career Opts        1\n",
              "Project for College project work      1\n",
              "DL, ML                                1\n",
              "Name: count, Length: 4046, dtype: int64"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['What are you looking for in Product ?'].value_counts()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "27ccc7eb",
      "metadata": {
        "id": "27ccc7eb"
      },
      "source": [
        "**Observation**\n",
        "* This feature has many unqiue values and processing it will take a lot of time"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "cc162624",
      "metadata": {
        "id": "cc162624"
      },
      "source": [
        "**Think about it**\n",
        "* Should we still work with this column?\n",
        "* If yes, what can we do?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "0a7fdb11",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0a7fdb11",
        "outputId": "2e80d0e9-7fe9-41b4-f29e-0411f6270852"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([nan, 'Sales lead', 'Start Project', 'Demo button lead',\n",
              "       'Chat lead', 'Cashback lead', 'eBook',\n",
              "       'Demo button lead, Chat lead', 'Sales lead, Demo button lead',\n",
              "       'Sales lead, Chat lead', 'Sales lead, eBook'], dtype=object)"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['Website Source'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "43c6ffdb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43c6ffdb",
        "outputId": "e6b7dc64-c268-43f9-e804-ff330c8c131b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Website Source\n",
              "Sales lead                      23121\n",
              "Start Project                     560\n",
              "Demo button lead                  267\n",
              "Chat lead                         114\n",
              "Cashback lead                      10\n",
              "eBook                               5\n",
              "Sales lead, Demo button lead        5\n",
              "Sales lead, Chat lead               3\n",
              "Sales lead, eBook                   2\n",
              "Demo button lead, Chat lead         1\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['Website Source'].value_counts()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "09318ca6",
      "metadata": {
        "id": "09318ca6"
      },
      "source": [
        "**Observation**\n",
        "* Column has very less variance in terms of frequency\n",
        "* Most of the values are concentrated around 1 or 2 enums"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "ace526a6",
      "metadata": {
        "id": "ace526a6"
      },
      "source": [
        "**Think about it**\n",
        "* Almost no variance in data, can model learn something important?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "c0cc98be",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0cc98be",
        "outputId": "2062fb74-3eec-4f80-b5e6-b884753e662d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Marketing Source\n",
              "SEO                                            10127\n",
              "Paid - Instagram                                3895\n",
              "Paid-Adwords                                    3514\n",
              "Paid-YouTube                                    2652\n",
              "Affiliate                                       2531\n",
              "Medium                                          2215\n",
              "Paid - Facebook                                 1528\n",
              "Email Campaign                                  1050\n",
              "Paid - Linkedin                                  154\n",
              "Naukri                                           102\n",
              "Medium, Paid-Adwords                              70\n",
              "SEO, Medium, Paid-Adwords                         57\n",
              "Referral                                          50\n",
              "SEO, Affiliate                                    48\n",
              "Linkedin jobs                                     46\n",
              "SEO, Paid-Adwords                                 44\n",
              "SEO, Paid - Instagram                             34\n",
              "Affiliate, Medium                                 29\n",
              "SEO, Medium                                       26\n",
              "Paid - Instagram, Paid-Adwords                    24\n",
              "Paid-Adwords, Paid-YouTube                        20\n",
              "SEO, Paid-YouTube                                 16\n",
              "SEO, Paid - Facebook                              15\n",
              "Affiliate, Paid-Adwords                           14\n",
              "SEO, Paid - Instagram, Paid-Adwords               11\n",
              "SEO, Linkedin jobs                                 8\n",
              "Paid - Facebook, Paid-Adwords                      8\n",
              "SEO, Paid - Facebook, Paid-Adwords                 7\n",
              "SEO, Paid - Linkedin                               6\n",
              "SEO, Naukri                                        6\n",
              "Affiliate, Email Campaign                          4\n",
              "SEO, Paid - Instagram, Medium                      4\n",
              "Paid - Linkedin, Paid-Adwords                      3\n",
              "Paid - Instagram, Medium                           3\n",
              "Affiliate, Paid-YouTube                            3\n",
              "Paid - Facebook, Paid - Instagram                  2\n",
              "Paid-Adwords, Email Campaign                       2\n",
              "Medium, Paid-YouTube                               2\n",
              "SEO, Email Campaign                                2\n",
              "Paid - Linkedin, Medium                            1\n",
              "SEO, Paid - Instagram, Medium, Paid-Adwords        1\n",
              "Medium, Paid-Adwords, Paid-YouTube                 1\n",
              "Paid-YouTube, Email Campaign                       1\n",
              "Paid - Linkedin, Affiliate                         1\n",
              "SEO, Paid - Facebook, Medium                       1\n",
              "SEO, Affiliate, Medium                             1\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['Marketing Source'].value_counts()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "4ee2037d",
      "metadata": {
        "id": "4ee2037d"
      },
      "source": [
        "**Observation**\n",
        "* There is a long tail of values\n",
        "* The 1st half looks really interesting in terms of distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "c9d9c2e1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9d9c2e1",
        "outputId": "61374b16-4141-4a6a-e158-fed423163e02"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Demo Status\n",
              "Scheduled    4000\n",
              "Done         3956\n",
              "No Show      3467\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['Demo Status'].value_counts()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "9883eec4",
      "metadata": {
        "id": "9883eec4"
      },
      "source": [
        "**Observation**\n",
        "* The column is nicely dsitributed\n",
        "* Has very less unique values"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "751f61e1",
      "metadata": {
        "id": "751f61e1"
      },
      "source": [
        "**Think about it**\n",
        "* If we use this column, are we doing a feature leak?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "5d203d71",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5d203d71",
        "outputId": "fc7711b9-4911-4a3c-a959-9f0996359463"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Lead Location(Manual)\n",
              "IN                       0.403900\n",
              "India                    0.322354\n",
              "In                       0.059444\n",
              "US                       0.046635\n",
              "in                       0.018128\n",
              "                           ...   \n",
              "Mumbai, India.           0.000029\n",
              "Hungary                  0.000029\n",
              "IN\\                      0.000029\n",
              "ps                       0.000029\n",
              "Vishakhapatnam, India    0.000029\n",
              "Name: proportion, Length: 415, dtype: float64"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['Lead Location(Manual)'].value_counts(normalize=1)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "4d96028e",
      "metadata": {
        "id": "4d96028e"
      },
      "source": [
        "**Observation**\n",
        "* This feature again has a very long tail"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "63efdc26",
      "metadata": {
        "id": "63efdc26"
      },
      "source": [
        "**Think about it**\n",
        "* What if we just create 2 enums; India and Non India"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "f74ed9ea",
      "metadata": {
        "id": "f74ed9ea"
      },
      "source": [
        "# **Data Processing & Feature engineering**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "d274025f",
      "metadata": {
        "id": "d274025f"
      },
      "source": [
        "### **Data Preprocessing and Leakage**\n",
        "\n",
        "Data leakage is a situation where information from the test or prediction data is inadvertently used during the training process of a machine learning model. This can occur when information from the test or prediction data is leaked into the training data, and the model uses this information to improve its performance during the training process.\n",
        "\n",
        "Data leakage can occur during the preprocessing phase of machine learning when information from the test or prediction data is used to preprocess the training data, inadvertently leaking information from the test or prediction data into the training data.\n",
        "\n",
        "For example, consider a scenario where the preprocessing step involves imputing missing values in the dataset. If the missing values are imputed using the mean or median values of the entire dataset, including the test and prediction data, then the imputed values in the training data may be influenced by the values in the test and prediction data. This can lead to data leakage, as the model may learn to recognize patterns in the test and prediction data during the training process, leading to overfitting and poor generalization performance.\n",
        "\n",
        "\n",
        "To avoid data leakage, it's important to perform the data preprocessing steps on the training data only, and then apply the same preprocessing steps to the test and prediction data separately. This ensures that the test and prediction data remain unseen by the model during the training process, and helps to prevent overfitting and improve the accuracy of the model.\n",
        "\n",
        "In the context of this problem, we will perform data preprocessing steps together for the sake of simplicity, which could potentially lead to data leakage. However, in real-world scenarios, it's important to treat the test and prediction data separately and apply the necessary preprocessing steps separately, based on the characteristics of the data."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "b5ae2ed8",
      "metadata": {
        "id": "b5ae2ed8"
      },
      "source": [
        "### **Missing Value Detection and Imputation**\n",
        "\n",
        "Real world datasets are never friendly to data scientists. They always pose great challenges to those who are dealing with them due to many different reasons and one of them is “missing values”\n",
        "\n",
        "Missing values can be imputed with a provided constant value, or using the statistics (mean, median or most frequent) of each column in which the missing values are located"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "51504bc4",
      "metadata": {
        "id": "51504bc4"
      },
      "source": [
        "We previously saw there are some missing values in the data. Lets have a look into that now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "34b49383",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34b49383",
        "outputId": "f04358ae-742a-40ba-d100-2ac6d523f853"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['Lead Owner'].isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "cbf04369",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbf04369",
        "outputId": "42528089-6070-4412-e69a-9be9834e41fd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "137"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['Interest Level'].isna().sum()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "a751c7cd",
      "metadata": {
        "id": "a751c7cd"
      },
      "source": [
        "Since target variable has missing values, we will drop such rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "1ee1e68d",
      "metadata": {
        "id": "1ee1e68d"
      },
      "outputs": [],
      "source": [
        "df = df[df['Interest Level'].notna()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "ca2d71e3",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape after drop: (38847, 16)\n"
          ]
        }
      ],
      "source": [
        "print(f\"Shape after drop: {df.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "1ae56795",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ae56795",
        "outputId": "40b846ae-570f-4b29-994c-62e5884c4fe5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Interest Level\n",
              "Slightly Interested    14572\n",
              "Not Interested         10545\n",
              "No Answer               9254\n",
              "Not called              1585\n",
              "Fairly Interested       1320\n",
              "Closed                   811\n",
              "Invalid Number           636\n",
              "Very Interested          124\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['Interest Level'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02e6096c",
      "metadata": {},
      "source": [
        "Checking for missing values in Lead Owner (none found) and Interest Level (137 missing). Since Interest Level is the target variable,  correctly drop those rows to avoid training on unlabeled data. Finally, checked the class distribution to assess potential imbalance for downstream modeling."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "ac69be7f",
      "metadata": {
        "id": "ac69be7f"
      },
      "source": [
        "##### Now we will handle our target variable\n",
        "\n",
        "Since there are multiple values in target variable and we want to formulate our problem as a binary classification problem, we will do the following assignments"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "e4da659a",
      "metadata": {
        "id": "e4da659a"
      },
      "source": [
        "**Label assignment:**\n",
        "* Slightly Interested = 1\n",
        "* Not Interested=0\n",
        "* No Answer=0\n",
        "* Fairly Interested=1\n",
        "* Very Interested=1\n",
        "\n",
        "* we will drop rows where value is Not called, Closed or Invalid Number"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "222c5cc0",
      "metadata": {
        "id": "222c5cc0"
      },
      "outputs": [],
      "source": [
        "df = df[~df['Interest Level'].isin([\"Not called\", \"Closed\", \"Invalid Number\"])]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "7cd536f8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7cd536f8",
        "outputId": "329b75ac-6383-406e-ab00-41d33a1ce798"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Interest Level\n",
              "Slightly Interested    14572\n",
              "Not Interested         10545\n",
              "No Answer               9254\n",
              "Fairly Interested       1320\n",
              "Very Interested          124\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['Interest Level'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "4a9bea5c",
      "metadata": {
        "id": "4a9bea5c"
      },
      "outputs": [],
      "source": [
        "df['Interest Level'] = df['Interest Level'].apply(lambda x: 1 if x in [\"Slightly Interested\", \"Fairly Interested\", \"Very Interested\"] else 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "cf0a06e1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cf0a06e1",
        "outputId": "f882d24b-1710-402c-9d6f-e33378127fa8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Interest Level\n",
              "0    19799\n",
              "1    16016\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['Interest Level'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c93f641",
      "metadata": {},
      "source": [
        "We cleaned and simplified the Interest Level column to make it suitable for binary classification. First, we removed entries that didn’t reflect actual interest, such as \"Not called\", \"Closed\", and \"Invalid Number\". Then, we grouped all genuinely interested responses (\"Slightly Interested\", \"Fairly Interested\", and \"Very Interested\") under a single class labeled 1, and everything else as 0. This allowed us to frame the problem as a clear interested vs. not interested prediction task, with a fairly balanced distribution across the two classes."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "7d6d1c15",
      "metadata": {
        "id": "7d6d1c15"
      },
      "source": [
        "#### Drop not imporant columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "12479545",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12479545",
        "outputId": "71c41024-aca3-4cbc-c302-877ef76ce2c2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['Lead Id', 'Lead Owner', 'Interest Level', 'Lead created',\n",
              "       'Lead Location(Auto)', 'Creation Source', 'Next activity',\n",
              "       'What do you do currently ?', 'What are you looking for in Product ?',\n",
              "       'Website Source', 'Lead Last Update time', 'Marketing Source',\n",
              "       'Lead Location(Manual)', 'Demo Date', 'Demo Status', 'Closure date'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "a61c7bce",
      "metadata": {
        "id": "a61c7bce"
      },
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "\"['Lead Id', 'Lead Location(Manual)', 'Demo Date', 'Demo Status', 'Closure date'] not found in axis\"",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[54], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLead Id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLead Location(Manual)\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDemo Date\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDemo Status\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mClosure date\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\SHYAMDEVADIGA\\miniconda3\\envs\\rag-app\\lib\\site-packages\\pandas\\core\\frame.py:5581\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   5433\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdrop\u001b[39m(\n\u001b[0;32m   5434\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   5435\u001b[0m     labels: IndexLabel \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5442\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   5443\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   5444\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   5445\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[0;32m   5446\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5579\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[0;32m   5580\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 5581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   5582\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5583\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5584\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5585\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5587\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5588\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5589\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\SHYAMDEVADIGA\\miniconda3\\envs\\rag-app\\lib\\site-packages\\pandas\\core\\generic.py:4788\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4786\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   4787\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 4788\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4790\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[0;32m   4791\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
            "File \u001b[1;32mc:\\Users\\SHYAMDEVADIGA\\miniconda3\\envs\\rag-app\\lib\\site-packages\\pandas\\core\\generic.py:4830\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[1;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[0;32m   4828\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   4829\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 4830\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m \u001b[43maxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4831\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[0;32m   4833\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[0;32m   4834\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\SHYAMDEVADIGA\\miniconda3\\envs\\rag-app\\lib\\site-packages\\pandas\\core\\indexes\\base.py:7070\u001b[0m, in \u001b[0;36mIndex.drop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   7068\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m   7069\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 7070\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels[mask]\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   7071\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[0;32m   7072\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
            "\u001b[1;31mKeyError\u001b[0m: \"['Lead Id', 'Lead Location(Manual)', 'Demo Date', 'Demo Status', 'Closure date'] not found in axis\""
          ]
        }
      ],
      "source": [
        "df = df.drop([\"Lead Id\", \"Lead Location(Manual)\", \"Demo Date\", \"Demo Status\", \"Closure date\"], axis=1)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "443b25ed",
      "metadata": {
        "id": "443b25ed"
      },
      "source": [
        "#### Lead creation time\n",
        "\n",
        "- We will create 2 features here from our lead creation time column\n",
        "    1. hour of day\n",
        "    2. day of week"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "d264b4d7",
      "metadata": {
        "id": "d264b4d7"
      },
      "outputs": [],
      "source": [
        "df['hour_of_day'] = df['Lead created'].dt.hour\n",
        "df['day_of_week'] = df['Lead created'].dt.weekday"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "4c036477",
      "metadata": {
        "id": "4c036477"
      },
      "outputs": [],
      "source": [
        "df = df.drop([\"Lead created\"], axis=1)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "19f4a98a",
      "metadata": {
        "id": "19f4a98a"
      },
      "source": [
        "Now lead created column is not useful and we drop it"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "b2f10f0e",
      "metadata": {
        "id": "b2f10f0e"
      },
      "source": [
        "#### Creation source"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "9c2c683c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9c2c683c",
        "outputId": "ea517c29-c100-4f8a-dec6-3d3ec53dafeb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Creation Source\n",
              "API                 33317\n",
              "Manually created     2346\n",
              "Deal                  152\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['Creation Source'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "e8a78803",
      "metadata": {
        "id": "e8a78803"
      },
      "outputs": [],
      "source": [
        "from pandas import factorize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "b3bd9c05",
      "metadata": {
        "id": "b3bd9c05"
      },
      "outputs": [],
      "source": [
        "labels, categories = factorize(df[\"Creation Source\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "077bad84",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "077bad84",
        "outputId": "65d7ca1f-dfb1-4643-bee5-0703f77dbcd6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.008490292073158507"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df[\"labels\"] = labels\n",
        "abs(df[\"Interest Level\"].corr(df[\"labels\"]))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "106e4d2e",
      "metadata": {
        "id": "106e4d2e"
      },
      "source": [
        "There is a positive correlation with the target variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "66a49fa2",
      "metadata": {
        "id": "66a49fa2"
      },
      "outputs": [],
      "source": [
        "df = df.drop([\"labels\"], axis=1)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "def6f681",
      "metadata": {
        "id": "def6f681"
      },
      "source": [
        "#### What do you do currently?\n",
        "\n",
        "* student = 1\n",
        "* others = 0\n",
        "\n",
        "As we saw earlier, this feature has a large number of values of which students are a dominating part.\n",
        "\n",
        "We will binarize this column into students and non-students"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "757bc0fd",
      "metadata": {
        "id": "757bc0fd"
      },
      "source": [
        "**Facts**\n",
        "\n",
        "<i>Binarization</i> is the process of dividing data into two groups and assigning one out. of two values to all the members of the same group. This is usually accomplished. by defining a threshold t and assigning the value 0 to all the data points below. the threshold and 1 to those above it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "aeaeba29",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aeaeba29",
        "outputId": "4724d979-ed61-4cce-9145-134eba541192"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "19419"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['What do you do currently ?'].isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "06d83d72",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06d83d72",
        "outputId": "ca8a56a4-f2cf-4f8f-890e-adce00346cee"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "What do you do currently ?\n",
              "Student                           0.205904\n",
              "student                           0.076482\n",
              "Fresher                           0.018053\n",
              "Working                           0.011710\n",
              "Data Engineer                     0.008295\n",
              "                                    ...   \n",
              "Completed mba in civil            0.000061\n",
              "Into automation                   0.000061\n",
              "Stu - Btech Final year            0.000061\n",
              "Big Data Prof                     0.000061\n",
              "Course AIMA - Business Analyst    0.000061\n",
              "Name: proportion, Length: 6592, dtype: float64"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['What do you do currently ?'].value_counts(normalize=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "8d78f5f5",
      "metadata": {
        "id": "8d78f5f5"
      },
      "outputs": [],
      "source": [
        "df['What do you do currently ?'] = df['What do you do currently ?'].apply(lambda x: 1 if 'student' in str(x).strip().lower() else 0)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "3f4cde99",
      "metadata": {
        "id": "3f4cde99"
      },
      "source": [
        "#### Website Source"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "b5162ba9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5162ba9",
        "outputId": "58ac2805-911e-481a-e625-a961b474e64d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "13828"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['Website Source'].isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "d2cc8280",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2cc8280",
        "outputId": "5b28dc85-dac7-46dd-b878-4d9ea02d6a51"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Website Source\n",
              "Sales lead                      21133\n",
              "Start Project                     517\n",
              "Demo button lead                  240\n",
              "Chat lead                          78\n",
              "Cashback lead                       6\n",
              "eBook                               5\n",
              "Sales lead, Demo button lead        4\n",
              "Sales lead, Chat lead               2\n",
              "Sales lead, eBook                   2\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['Website Source'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "a299a270",
      "metadata": {
        "id": "a299a270"
      },
      "outputs": [],
      "source": [
        "df = df.drop([\"Website Source\"], axis=1)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "5eeab9bf",
      "metadata": {
        "id": "5eeab9bf"
      },
      "source": [
        "Dropping the <b>Website Source</b> column as there is not enough variance"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "787507aa",
      "metadata": {
        "id": "787507aa"
      },
      "source": [
        "#### Marketing Source"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "58a6c1f6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58a6c1f6",
        "outputId": "0b995910-6c7f-4afd-c1f7-96585d1a7807"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Marketing Source\n",
              "SEO                                            9751\n",
              "Paid - Instagram                               3738\n",
              "Paid-Adwords                                   3258\n",
              "Paid-YouTube                                   2376\n",
              "Affiliate                                      2215\n",
              "Medium                                         2045\n",
              "Paid - Facebook                                1436\n",
              "Email Campaign                                  813\n",
              "Paid - Linkedin                                 136\n",
              "Naukri                                           99\n",
              "Medium, Paid-Adwords                             63\n",
              "SEO, Medium, Paid-Adwords                        51\n",
              "Linkedin jobs                                    42\n",
              "SEO, Paid-Adwords                                39\n",
              "Referral                                         38\n",
              "SEO, Affiliate                                   38\n",
              "SEO, Paid - Instagram                            30\n",
              "Affiliate, Medium                                27\n",
              "Paid - Instagram, Paid-Adwords                   23\n",
              "SEO, Medium                                      23\n",
              "Paid-Adwords, Paid-YouTube                       19\n",
              "Affiliate, Paid-Adwords                          14\n",
              "SEO, Paid - Facebook                             12\n",
              "SEO, Paid-YouTube                                12\n",
              "SEO, Paid - Instagram, Paid-Adwords              11\n",
              "Paid - Facebook, Paid-Adwords                     8\n",
              "SEO, Paid - Facebook, Paid-Adwords                6\n",
              "SEO, Linkedin jobs                                5\n",
              "SEO, Naukri                                       4\n",
              "SEO, Paid - Linkedin                              4\n",
              "SEO, Paid - Instagram, Medium                     4\n",
              "Paid - Linkedin, Paid-Adwords                     3\n",
              "Paid - Instagram, Medium                          3\n",
              "Paid - Facebook, Paid - Instagram                 2\n",
              "Affiliate, Email Campaign                         2\n",
              "Medium, Paid-YouTube                              2\n",
              "SEO, Email Campaign                               2\n",
              "Paid - Linkedin, Medium                           1\n",
              "SEO, Paid - Instagram, Medium, Paid-Adwords       1\n",
              "Paid-Adwords, Email Campaign                      1\n",
              "SEO, Paid - Facebook, Medium                      1\n",
              "SEO, Affiliate, Medium                            1\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['Marketing Source'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "13ac9b1f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13ac9b1f",
        "outputId": "16b4af38-da77-4b14-d3b4-80162ec68d07"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "9456"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['Marketing Source'].isna().sum()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "ebe26a5c",
      "metadata": {
        "id": "ebe26a5c"
      },
      "source": [
        "Marketing Source has a large number of missing value and it will be noisy if we do an imputation here.\n",
        "\n",
        "Rather, let's create a new value <b>Unknown</b> which will be substituted for NA values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "82e82302",
      "metadata": {
        "id": "82e82302"
      },
      "outputs": [],
      "source": [
        "df['Marketing Source'].fillna(\"Unknown\", inplace=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "5f0e27e8",
      "metadata": {
        "id": "5f0e27e8"
      },
      "source": [
        "PS: Imputation with Unknown led to improvements that dropping these rows"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "20cabce1",
      "metadata": {
        "id": "20cabce1"
      },
      "source": [
        "### Label Encoding"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "0e8dc34f",
      "metadata": {
        "id": "0e8dc34f"
      },
      "source": [
        "**Transforming Categorical Variables**\n",
        "\n",
        "Transforming variables is an important step in the data preprocessing pipeline of machine learning, as it helps to convert the data into a format that is suitable for analysis and modeling. There are several ways to transform variables, depending on the type and nature of the data.\n",
        "\n",
        "Categorical variables, for example, are variables that take on discrete values from a finite set of categories, such as colors, gender, or occupation. One common way to transform categorical variables is through one-hot encoding. One-hot encoding involves creating a new binary variable for each category in the original variable, where the value is 1 if the observation belongs to that category and 0 otherwise. This approach is useful when the categories have no natural order or ranking.\n",
        "\n",
        "Another way to transform categorical variables is through label encoding. Label encoding involves assigning a unique integer value to each category in the variable. This approach is useful when the categories have a natural order or ranking, such as low, medium, and high.\n",
        "Transforming categorical features into numerical labels:\n",
        "\n",
        "**Note:** We are NOT using dummies here to minimize the explosion of columns because of the distance methods we are using.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "0bad061b",
      "metadata": {
        "id": "0bad061b"
      },
      "outputs": [],
      "source": [
        "label_encoder1 = preprocessing.LabelEncoder()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "c0580d1e",
      "metadata": {
        "id": "c0580d1e"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'save_point' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[53], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMarketing Source\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39m label_encoder1\u001b[38;5;241m.\u001b[39mfit_transform(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMarketing Source\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m----> 2\u001b[0m \u001b[43msave_point\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfcMar1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'save_point' is not defined"
          ]
        }
      ],
      "source": [
        "df['Marketing Source']= label_encoder1.fit_transform(df['Marketing Source'])\n",
        "save_point(\"fcMar1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8ca91f8",
      "metadata": {
        "id": "c8ca91f8"
      },
      "outputs": [],
      "source": [
        "label_encoder2 = preprocessing.LabelEncoder()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee2ddc24",
      "metadata": {
        "id": "ee2ddc24"
      },
      "outputs": [],
      "source": [
        "df['Lead Owner']= label_encoder2.fit_transform(df['Lead Owner'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c191d4c",
      "metadata": {
        "id": "5c191d4c"
      },
      "outputs": [],
      "source": [
        "label_encoder3 = preprocessing.LabelEncoder()\n",
        "df['Creation Source']= label_encoder3.fit_transform(df['Creation Source'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b06bbf2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "1b06bbf2",
        "outputId": "10026ea1-1f28-4d99-82bf-74c5f7ffba4d"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "0ee8a1c7",
      "metadata": {
        "id": "0ee8a1c7"
      },
      "source": [
        "We transformed 3 columns using label encoding\n",
        "\n",
        "Remember one thing, you should always use the same label encoding variable for test dataset. Since here we are handling train/test together, we are not worrying about it"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "5c348cd6",
      "metadata": {
        "id": "5c348cd6"
      },
      "source": [
        "# **Model Building and Testing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cbb206b5",
      "metadata": {
        "id": "cbb206b5"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_recall_curve, roc_curve, plot_roc_curve, plot_precision_recall_curve\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "ab5cb15e",
      "metadata": {
        "id": "ab5cb15e"
      },
      "source": [
        "Identify the right features for the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f22e051",
      "metadata": {
        "id": "5f22e051"
      },
      "outputs": [],
      "source": [
        "X = df[[\"Lead Owner\", \"What do you do currently ?\", \"Marketing Source\", \"Creation Source\", \"hour_of_day\", \"day_of_week\"]]\n",
        "y = df[\"Interest Level\"]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "d16b1e64",
      "metadata": {
        "id": "d16b1e64"
      },
      "source": [
        "**Splitting the dataset into a training and production dataset:**\n",
        "\n",
        "- Training: Part of data used for training our supervised models\n",
        "- Test: Part of the dataset used for testing our models performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5ee0e32",
      "metadata": {
        "id": "e5ee0e32"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be779820",
      "metadata": {
        "id": "be779820",
        "outputId": "ca9c76cf-1e5c-436c-c969-418214caff0b"
      },
      "outputs": [],
      "source": [
        "X_train.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "86132c80",
      "metadata": {
        "id": "86132c80"
      },
      "source": [
        "We finally have prepared model ready data.\n",
        "\n",
        "Lets look into model building now."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "1dd5fa7c",
      "metadata": {
        "id": "1dd5fa7c"
      },
      "source": [
        "## **Supervised learning**\n",
        "\n",
        "\n",
        "\n",
        "Supervised learning uses a training set to teach models to yield the desired output. This training dataset includes inputs and correct outputs, which allow the model to learn over time. The algorithm measures its accuracy through the loss function, adjusting until the error has been sufficiently minimized.\n",
        "\n",
        "Supervised learning can be separated into two types of problems when data mining—classification and regression:\n",
        "\n",
        "1. Classification uses an algorithm to accurately assign test data into specific categories. It recognizes specific entities within the dataset and attempts to draw some conclusions on how those entities should be labeled or defined. Common classification algorithms are linear classifiers, support vector machines (SVM), decision trees, k-nearest neighbor, and random forest, which are described in more detail below.\n",
        "\n",
        "\n",
        "2. Regression is used to understand the relationship between dependent and independent variables. It is commonly used to make projections, such as for sales revenue for a given business. Linear regression, logistical regression, and polynomial regression are popular regression algorithms.\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "da881f41",
      "metadata": {
        "id": "da881f41"
      },
      "source": [
        "## **Decision Tree**\n",
        "\n",
        "\n",
        "\n",
        "A decision tree is a non-parametric supervised learning algorithm, which is utilized for both classification and regression tasks. It has a hierarchical, tree structure, which consists of a root node, branches, internal nodes and leaf nodes.\n",
        "\n",
        "\n",
        "Decision tree learning employs a divide and conquer strategy by conducting a greedy search to identify the optimal split points within a tree. This process of splitting is then repeated in a top-down, recursive manner until all, or the majority of records have been classified under specific class labels. Whether or not all data points are classified as homogenous sets is largely dependent on the complexity of the decision tree. Smaller trees are more easily able to attain pure leaf nodes—i.e. data points in a single class.\n",
        "\n",
        "However, as a tree grows in size, it becomes increasingly difficult to maintain this purity, and it usually results in too little data falling within a given subtree. When this occurs, it is known as data fragmentation, and it can often lead to overfitting. As a result, decision trees have preference for small trees, which is consistent with the principle of parsimony in Occam’s Razor; that is, “entities should not be multiplied beyond necessity.” Said differently, decision trees should add complexity only if necessary, as the simplest explanation is often the best. To reduce complexity and prevent overfitting, pruning is usually employed; this is a process, which removes branches that split on features with low importance. The model’s fit can then be evaluated through the process of cross-validation."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "6b6d0234",
      "metadata": {},
      "source": [
        "### **Bagging**\n",
        "\n",
        "\n",
        "\n",
        "Bagging is an ensemble learning technique that aims to decrease the variance of a single estimator by combining the predictions from multiple learners. The basic idea behind bagging is to generate multiple versions of the training dataset through random sampling with replacement, and then train a separate classifier for each sampled dataset. The predictions from these individual classifiers are then combined using averaging or voting to obtain a final prediction.\n",
        "\n",
        "**Algorithm:**\n",
        "\n",
        "Suppose we have a training set D of size n, and we want to train a classifier using bagging. Here are the steps involved:\n",
        "\n",
        "* Create k different bootstrap samples from D, each of size n.\n",
        "* Train a classifier on each bootstrap sample.\n",
        "* When making predictions on a new data point, take the average or majority vote of the predictions from each of the k classifiers.\n",
        "\n",
        "\n",
        "**Mathematical Explanation:**\n",
        "\n",
        "Suppose we have a binary classification problem with classes -1 and 1. Let's also assume that we have a training set D of size n, and we want to train a decision tree classifier using bagging.\n",
        "\n",
        "**Bootstrap Sample**: For each of the k classifiers, we create a bootstrap sample of size n by sampling with replacement from D. This means that each bootstrap sample may contain duplicates of some instances and may also miss some instances from the original dataset. Let's denote the i-th bootstrap sample as D_i.\n",
        "\n",
        "**Train a Classifier**: We train a decision tree classifier T_i on each bootstrap sample D_i. This gives us k classifiers T_1, T_2, ..., T_k.\n",
        "\n",
        "**Combine Predictions**: To make a prediction on a new data point x, we take the majority vote of the predictions from each of the k classifiers.\n",
        "\n",
        "The idea behind bagging is that the variance of the prediction error decreases as k increases. This is because each classifier has a chance to explore a different part of the feature space due to the random sampling with replacement, and the final prediction is a combination of these diverse classifiers.\n",
        "\n",
        "\n",
        "\n",
        "### **Boosting**\n",
        "\n",
        "Boosting is a machine learning algorithm that works by combining several weak models (also known as base learners) into a strong model. The goal of boosting is to reduce the bias and variance of the base learners by iteratively adding new models to the ensemble that focus on correcting the errors made by the previous models. In other words, the boosting algorithm tries to learn from the mistakes of the previous models and improve the overall accuracy of the ensemble.\n",
        "\n",
        "Boosting works by assigning higher weights to the data points that the previous models misclassified, and lower weights to the ones that were classified correctly. This ensures that the new model focuses more on the difficult data points that the previous models struggled with, and less on the ones that were already well-classified. As a result, the new model is more specialized and can improve the accuracy of the ensemble.\n",
        "\n",
        "There are several types of boosting algorithms, including AdaBoost (Adaptive Boosting), Gradient Boosting, and XGBoost (Extreme Gradient Boosting). Each of these algorithms has its own approach to assigning weights to the data points and building the new models, but they all share the fundamental idea of iteratively improving the accuracy of the ensemble by combining weak models into a strong one. Boosting is a powerful algorithm that has been shown to achieve state-of-the-art results in many machine learning tasks, such as image classification, natural language processing, and recommender systems.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Difference between Bagging and Boosting**\n",
        "\n",
        "\n",
        "It's important to remember that boosting is a generic method, not a specific model, in order to comprehend it. Boosting involves specifying a weak model, such as regression or decision trees, and then improving it. In Ensemble Learning, the primary difference between Bagging and Boosting is that in bagging, weak learners are trained in simultaneously, but in boosting, they are trained sequentially. This means that each new model iteration increases the weights of the prior model's misclassified data. This redistribution of weights aids the algorithm in determining which parameters it should focus on in order to increase its performance.\n",
        "\n",
        "Both the Ensemble techniques are used in a different way as well.  Bagging methods, for example, are often used on poor learners who have large variance and low bias such as decision trees because they tend to overfit, whereas boosting methods are employed when there is low variance and high bias. While bagging can help prevent overfitting, boosting methods are more vulnerable to it because of a simple fact they continue to build on weak learners and continue to minimise error. This can lead to overfitting on the training data but specifying a decent number of models to be generated or hyperparameter tuning,  regularization can help in this case, if overfitting encountered.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "6438753e",
      "metadata": {},
      "source": [
        "### **Random Forest**\n",
        "\n",
        "\n",
        "Another way that decision trees can maintain their accuracy is by forming an ensemble via a random forest algorithm; this classifier predicts more accurate results, particularly when the individual trees are uncorrelated with each other.\n",
        "\n",
        "Random Forest is an ensemble learning algorithm that builds a large number of decision trees and combines them to make a final prediction. It is a type of bagging method, where multiple decision trees are trained on random subsets of the training data and features. The algorithm then averages the predictions of these individual trees to produce a final prediction. Random Forest is particularly useful for handling high-dimensional data and for avoiding overfitting.\n",
        "\n",
        "**Algorithm of Random Forest**\n",
        "\n",
        "The algorithm of Random Forest can be summarized in the following steps:\n",
        "\n",
        "* Start by randomly selecting a subset of the training data, with replacement. This subset is called the bootstrap sample.\n",
        "\n",
        "* Next, randomly select a subset of features from the full feature set.\n",
        "\n",
        "* Build a decision tree using the bootstrap sample and the selected subset of features. At each node of the tree, select the best feature and split the data based on the selected feature.\n",
        "\n",
        "* Repeat steps 1-3 to build multiple trees.\n",
        "\n",
        "* Finally, combine the predictions of all trees to make a final prediction. For classification, this is usually done by taking a majority vote of the predicted classes. For regression, this is usually done by taking the average of the predicted values.\n",
        "\n",
        "\n",
        "**Mathematics Behind Random Forest**\n",
        "\n",
        "The mathematics behind Random Forest involves the use of decision trees and the bootstrap sampling technique. Decision trees are constructed using a recursive binary partitioning algorithm that splits the data based on the values of the selected features. At each node, the algorithm chooses the feature and the split point that maximizes the information gain. Information gain measures the reduction in entropy or impurity of the target variable after the split. The goal is to minimize the impurity of the subsets after each split.\n",
        "\n",
        "Bootstrap sampling is a statistical technique that involves randomly sampling the data with replacement to create multiple subsets. These subsets are used to train individual decision trees. By using bootstrap samples, the algorithm can generate multiple versions of the same dataset with slightly different distributions. This introduces randomness into the training process, which helps to reduce overfitting.\n",
        "\n",
        "\n",
        "\n",
        "**Difference between Bagging and Random Forest**\n",
        "\n",
        "Bagging and Random Forest are both ensemble learning algorithms that involve training multiple models on random subsets of the data. The main difference between the two is the way the individual models are trained.\n",
        "\n",
        "Bagging involves training multiple models using the bootstrap sampling technique, but each model uses the same set of features. This can lead to correlated predictions, which reduces the variance but not necessarily the bias of the model.\n",
        "\n",
        "Random Forest, on the other hand, involves training multiple models using the bootstrap sampling technique, but each model uses a randomly selected subset of features. This introduces additional randomness into the model and helps to reduce the correlation between individual predictions. Random Forest can achieve better performance than Bagging, especially when dealing with high-dimensional data or noisy features. In simpler terms it uses subsets of observations as well as features.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "9b4cb623",
      "metadata": {
        "id": "9b4cb623"
      },
      "source": [
        "## **Gradient Boosting Trees**\n",
        "\n",
        "Gradient boosting is a machine learning technique used in regression and classification tasks, among others. It gives a prediction model in the form of an ensemble of weak prediction models, which are typically decision trees.\n",
        "\n",
        "When a decision tree is the weak learner, the resulting algorithm is called gradient-boosted trees; it usually outperforms random forest. A gradient-boosted trees model is built in a stage-wise fashion as in other boosting methods, but it generalizes the other methods by allowing optimization of an arbitrary differentiable loss function.\n",
        "\n",
        "Few examples of gradient boosting trees are Xgboost, LightGBM, etc"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "7a5e21ad",
      "metadata": {
        "id": "7a5e21ad"
      },
      "source": [
        "## **Bagging vs Boosting**\n",
        "\n",
        "\n",
        "1. Bagging: It is a homogeneous weak learners’ model that learns from each other independently in parallel and combines them for determining the model average.\n",
        "\n",
        "![image.png](https://media.geeksforgeeks.org/wp-content/uploads/20210707140912/Bagging.png)\n",
        "\n",
        "\n",
        "2. Boosting: It is also a homogeneous weak learners’ model but works differently from Bagging. In this model, learners learn sequentially and adaptively to improve model predictions of a learning algorithm.\n",
        "\n",
        "![image.png](https://media.geeksforgeeks.org/wp-content/uploads/20210707140911/Boosting.png)\n",
        "\n",
        "Credit: geeksforgeeks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "876217ff",
      "metadata": {
        "id": "876217ff"
      },
      "outputs": [],
      "source": [
        "rf = RandomForestClassifier(n_estimators=300)\n",
        "xgb = XGBClassifier(n_estimators=300, objective='binary:logistic', tree_method='hist', eta=0.1, max_depth=3)\n",
        "lgb = LGBMClassifier(n_estimators=300)\n",
        "checkpoint(\"fcMar1\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "21a59533",
      "metadata": {
        "id": "21a59533"
      },
      "source": [
        "#### Training multiple models together"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44028c29",
      "metadata": {
        "id": "44028c29",
        "outputId": "60050c36-4e72-4eff-d3bc-08a4a258ea27"
      },
      "outputs": [],
      "source": [
        "rf.fit(X_train, y_train)\n",
        "xgb.fit(X_train, y_train)\n",
        "lgb.fit(X_train, y_train)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "311e02e5",
      "metadata": {
        "id": "311e02e5"
      },
      "source": [
        "All models are trained very qucikly here.\n",
        "\n",
        "Scitkit-learn provides an additional parameter n_jobs=-1 which parallelize some of the models using cpu threads"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "d630bb00",
      "metadata": {
        "id": "d630bb00"
      },
      "source": [
        "# Model Evaluation"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "e023f4dd",
      "metadata": {},
      "source": [
        "### **Classification Evaluation Metrics**\n",
        "\n",
        "Classification evaluation metrics are used to evaluate the performance of a machine learning model that is trained for classification tasks. Some of the commonly used classification evaluation metrics are F1 score, recall score, confusion matrix, and ROC AUC score. Here's an overview of each of these metrics:\n",
        "\n",
        "**F1 score**: The F1 score is a metric that combines the precision and recall of a model into a single value. It is calculated as the harmonic mean of precision and recall, and is expressed as a value between 0 and 1, where 1 indicates perfect precision and recall.\n",
        "F1 score is the harmonic mean of precision and recall. It is calculated as follows:\n",
        "$$ F1 = \\frac{2}{\\frac{1}{precision} + \\frac{1}{recall}} $$\n",
        "where precision is the number of true positives divided by the sum of true positives and false positives, and recall is the number of true positives divided by the sum of true positives and false negatives.\n",
        "\n",
        "**Recall**: Use the recall score when the cost of false negatives (i.e., missing instances of a class) is high. For example, in a medical diagnosis problem, the cost of missing a positive case may be high, so recall would be a more appropriate metric.\n",
        "Recall score (also known as sensitivity) is the number of true positives divided by the sum of true positives and false negatives. It is given by the following formula:\n",
        "$$ Recall = \\frac{TP}{TP + FN} $$\n",
        "\n",
        "**Precision**: Precision is another important classification evaluation metric, which is defined as the ratio of true positives to the total predicted positives. It measures the accuracy of positive predictions made by the classifier, i.e., the proportion of positive identifications that were actually correct.\n",
        "The formula for precision is:\n",
        "$$ precision = \\frac{true\\ positive}{true\\ positive + false\\ positive} $$\n",
        "where true positive refers to the cases where the model correctly predicted the positive class, and false positive refers to the cases where the model incorrectly predicted the positive class.\n",
        "Precision is useful when the cost of false positives is high, such as in medical diagnosis or fraud detection, where a false positive can have serious consequences. In such cases, a higher precision indicates that the model is better at identifying true positives and minimizing false positives.\n",
        "\n",
        "**Confusion Matrix**:\n",
        "A confusion matrix is a table that is often used to describe the performance of a classification model. It compares the predicted labels with the true labels and counts the number of true positives, false positives, true negatives, and false negatives. Here is an example of a confusion matrix:\n",
        "\n",
        "|          | Actual Positive | Actual Negative |\n",
        "|----------|----------------|----------------|\n",
        "| Predicted Positive | True Positive (TP) | False Positive (FP) |\n",
        "| Predicted Negative | False Negative (FN) | True Negative (TN) |\n",
        "\n",
        "​\n",
        "\n",
        "\n",
        "\n",
        "**ROC AUC Score**:\n",
        "ROC AUC (Receiver Operating Characteristic Area Under the Curve) score is a measure of how well a classifier is able to distinguish between positive and negative classes. It is calculated as the area under the ROC curve. The ROC curve is created by plotting the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings. TPR is the number of true positives divided by the sum of true positives and false negatives, and FPR is the number of false positives divided by the sum of false positives and true negatives.\n",
        "$$ ROC\\ AUC\\ Score = \\int_0^1 TPR(FPR^{-1}(t)) dt $$\n",
        "where $FPR^{-1}$ is the inverse of the FPR function.\n",
        "\n",
        "**When to use which**:\n",
        "\n",
        "The choice of evaluation metric depends on the specific requirements of the business problem. Here are some general guidelines:\n",
        "\n",
        "* F1 score: Use the F1 score when the class distribution is imbalanced, and when both precision and recall are equally important.\n",
        "\n",
        "* Recall score: Use the recall score when the cost of false negatives (i.e., missing instances of a class) is high. For example, in a medical diagnosis problem, the cost of missing a positive case may be high, so recall would be a more appropriate metric.\n",
        "\n",
        "* Precision: Precision is useful when the cost of false positives is high, such as in medical diagnosis or fraud detection, where a false positive can have serious consequences. In such cases, a higher precision indicates that the model is better at identifying true positives and minimizing false positives.\n",
        "\n",
        "* Confusion matrix: The confusion matrix is a versatile tool that can be used to visualize the performance of a model across different classes. It can be useful for identifying specific areas of the model that need improvement.\n",
        "\n",
        "* ROC AUC score: Use the ROC AUC score when the ability to distinguish between positive and negative classes is important. For example, in a credit scoring problem, the ability to distinguish between good and bad credit risks is crucial.\n",
        "\n",
        "Importance with respect to the business problem:\n",
        "\n",
        "The importance of each evaluation metric varies depending on the business problem. For example, in a spam detection problem, precision may be more important than recall, since false positives (i.e., classifying a non-spam email as spam) may annoy users, while false negatives (i.e., missing a spam email) may not be as harmful. On the other hand, in a disease diagnosis problem, recall may be more important than precision, since missing a positive case (i.e., a false negative) could have serious consequences. Therefore, it is important to choose the evaluation metric that is most relevant to the specific business problem at hand.\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "ac8f43c8",
      "metadata": {
        "id": "ac8f43c8"
      },
      "source": [
        "### Evaluation metrics\n",
        "\n",
        "![image.png](https://blog.paperspace.com/content/images/2020/09/Fig01.jpg)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "efd4f662",
      "metadata": {
        "id": "efd4f662"
      },
      "source": [
        "1. Accuracy\n",
        "\n",
        "\\begin{equation}\n",
        "\\text{Accuracy} = \\frac{True Positive + True Negative}{True Positive + False Positive + True Negative + False Negative}\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "2. Precision\n",
        "\n",
        "\\begin{equation}\n",
        "\\text{Precision} = \\frac{True Positive}{True Positive + False Positive}\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "3. Recall\n",
        "\n",
        "\\begin{equation}\n",
        "\\text{Recall} = \\frac{True Positive}{True Positive + False Negative}\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "4. F1-score\n",
        "\n",
        "\\begin{equation}\n",
        "\\text{F1-score} = 2 * \\frac{Precision * Recall}{Precision + Recall}\n",
        "\\end{equation}"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "dda343ec",
      "metadata": {},
      "source": [
        "Precision-Recall (PR) curve and Area Under the Curve (AUC) curve are evaluation metrics commonly used in binary classification problems to assess the performance of a model and determine an appropriate threshold for decision making.\n",
        "\n",
        "The Precision-Recall (PR) curve is a graphical representation of the trade-off between precision and recall for different classification thresholds. Precision is the ratio of true positive predictions to the total number of positive predictions, while recall (also known as sensitivity or true positive rate) is the ratio of true positive predictions to the total number of actual positive instances in the data. The PR curve plots precision on the y-axis and recall on the x-axis, with each point on the curve representing a different classification threshold. A higher precision and recall indicate better model performance.\n",
        "\n",
        "The Area Under the Curve (AUC) is a single scalar value that summarizes the PR curve. It measures the overall performance of the model across all possible classification thresholds. The AUC value ranges from 0 to 1, where a higher value indicates better model performance. An AUC of 1 represents a perfect model that achieves maximum precision and recall across all thresholds.\n",
        "\n",
        "Choosing the right threshold depends on the specific requirements of your problem. The PR curve can help you visualize the precision-recall trade-off at different thresholds. If your problem prioritizes precision (minimizing false positives), you may want to choose a threshold that maximizes precision while maintaining a reasonable level of recall. On the other hand, if recall is more important (minimizing false negatives), you would choose a threshold that maximizes recall while still maintaining an acceptable level of precision.\n",
        "\n",
        "The selection of the threshold ultimately depends on the cost or impact of false positives and false negatives in your specific problem domain. By analyzing the PR curve and considering the specific requirements and trade-offs of your problem, you can make an informed decision about the threshold that best balances precision and recall for your particular use case.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d82e76a",
      "metadata": {
        "id": "7d82e76a"
      },
      "outputs": [],
      "source": [
        "def get_evaluation_metrics(model_name, model, pred, actual):\n",
        "    print(\"Accuracy of %s: \" % model_name, accuracy_score(pred, actual))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a29ac28",
      "metadata": {
        "id": "4a29ac28",
        "outputId": "8fe9aff2-192b-4e67-d349-b62f15ee6f16"
      },
      "outputs": [],
      "source": [
        "get_evaluation_metrics(\"Random Forest\", rf, rf.predict(X_test), y_test)\n",
        "get_evaluation_metrics(\"XGBoost\", xgb, xgb.predict(X_test), y_test)\n",
        "get_evaluation_metrics(\"Light GBM\", lgb, lgb.predict(X_test), y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09274f1e",
      "metadata": {
        "id": "09274f1e",
        "outputId": "1456a65a-745b-45c9-94e7-9a843acd3a5e"
      },
      "outputs": [],
      "source": [
        "plot_precision_recall_curve(rf, X_test, y_test)\n",
        "plot_roc_curve(rf, X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d6d2332",
      "metadata": {
        "id": "8d6d2332",
        "outputId": "0d418771-e2dd-43dc-a786-6786b43e1f09"
      },
      "outputs": [],
      "source": [
        "plot_precision_recall_curve(xgb, X_test, y_test)\n",
        "plot_roc_curve(xgb, X_test, y_test)\n",
        "checkpoint(\"fcMar1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6de17ac9",
      "metadata": {
        "id": "6de17ac9",
        "outputId": "edf9b4eb-44ee-4127-b7f0-fac7f03f6d03"
      },
      "outputs": [],
      "source": [
        "plot_precision_recall_curve(lgb, X_test, y_test)\n",
        "plot_roc_curve(lgb, X_test, y_test)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "5258426c",
      "metadata": {
        "id": "5258426c"
      },
      "source": [
        "### **Think about it**\n",
        "\n",
        "- Although numerically results are similar for XGBoost and LightGBM, which one do you think is better?\n",
        "- Why does PR curve matter here? Think in terms of business justfication!"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "133e3eaf",
      "metadata": {
        "id": "133e3eaf"
      },
      "source": [
        "# **Try it out**\n",
        "\n",
        "- Can you try out grid-search to tune hyperparameters for these models?\n",
        "\n",
        "- Can you come up with the right thresholds for these models depending on what do you feel is more important here? Precision or recall?\n",
        "\n",
        "- Can you train a multi-layer perceptron and check the performance?"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "131ca905",
      "metadata": {
        "id": "131ca905"
      },
      "source": [
        "## **Conclusion**\n",
        "\n",
        "In this project we used a bunch of supervised models to predict if the customer would be interested in a lead.\n",
        "\n",
        "The problem could have been formulated as a multi-class classification problem but we instead formulated this as a binary classification problem and the confidence on the predictions would enable the stakeholders to chase the lead.\n",
        "\n",
        "\n",
        "A successful data science project requires a clear understanding of the business problem and the data available, as well as the ability to select and apply appropriate data preprocessing techniques, feature engineering methods, and machine learning algorithms. It is also important to assess and optimize the performance of the model and communicate the results effectively to stakeholders.\n",
        "\n",
        "After looking at the PR and ROC curves above, we can conclude that <b>LightGBM</b> is giving us the best possible results."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "71eda412",
      "metadata": {
        "id": "71eda412"
      },
      "source": [
        "## **Interview Questions**\n",
        "\n",
        "### **Supervised Learning:**\n",
        "\n",
        "* What are decision trees? How does the model decide on the split?\n",
        "* What is boostrap aggregation?\n",
        "* Explain bias and variance in context of boosting and bagging?\n",
        "* How can you use decision tree for missing value imputation?\n",
        "* How does regularization work in gradient boosting models?\n",
        "\n",
        "\n",
        "### **Code Implementation:**\n",
        "\n",
        "* Is bagging or boosting computationally faster?\n",
        "* Can you write your own code to calculate precision and recall?\n",
        "* How would you handle dataset if the target variable would have been imbalanced?"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "94ee13fe",
      "metadata": {},
      "source": [
        "## **Feedback**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50962f33",
      "metadata": {
        "id": "50962f33"
      },
      "outputs": [],
      "source": [
        "feedback()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ced09f88",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "rag-app",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
